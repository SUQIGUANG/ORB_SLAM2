# ORBSLAM2论文

论文下载：https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7946260

论文翻译：http://www.sohu.com/a/154011668_715754



## 摘要

ORB-SLAM2是基于单目、双目和RGB-D相机的一套完整的SLAM方案。它能够实现地图重用、回环检测和重新定位的功能。无论是在室内的小型手持设备，还是到工厂环境的无人机和城市里驾驶的汽车，ORB-SLAM2都能够在标准的CPU上进行实时工作。ORB-SLAM2在**后端**上采用的是**基于单目和双目的光束法平差优化（BA）**的方式，这个方法允许”米“级别的轨迹精确度评估。此外，ORB-SLAM2包含一个轻量级的定位模式，该模式能够在允许零点漂移的条件下，利用视觉里程计来追踪未建图的区域并且匹配特征点。

我们用29个广泛使用的公共数据测试的结果显示，在大多数情况下，本文方案比此前方案精度更高，此外，我们开源了ORB-SLAM2源代码，不仅仅是为了整个SLAM领域，同时也希望能够为其他领域研究者提供一套SLAM的解决方案。

## I. 引言

SLAM（同时定位与地图重建）在过去的20年中，一直是计算机视觉和机器人领域的热门话题，同时也吸引了很多高科技公司的关注。SLAM技术是在未知的环境当中建立一个地图并且能够在地图当中实时的定位。在不同类型的传感器当中，相机十分廉价，并且能够提供丰富的环境信息，受到研究者的青睐。相机提供的图像信息可以用作鲁棒的和精确的位置识别。位置识别是SLAM系统中回环检测的关键模块（例如，当传感器检测到一个已经建好图的位置的时候，可以进行修正在探索过程中的误差）以及，能够修正由于剧烈的震动或者在系统进行初始化的时候在相机跟踪失败后的重新定位。因此以相机为核心的视觉SLAM在过去的一年中得到快速的发展。

视觉SLAM仅仅通过一个单目相机就能够完成。单目相机也是最便宜也是最小巧的传感器设备。然而深度信息无法从单目相机中观测到，地图的尺度和预测轨迹是未知的。此外，由于不能从第一帧当中进行三角测量化，单目视觉SLAM系统的启动往往需要多个视角或者滤波技术才能产生一个初始化的地图。最后，单目SLAM可能会造成尺度漂移,以及在探索的过程中执行纯旋转的时候可能会失败。通过使用一个双目或者RGB-D相机将会解决这些问题，并且能够成为一种更加有效的视觉SLAM的解决方案。

在这篇文章当中，我们在单目ORB-SLAM[1]的基础上提出ORB-SLAM2，有以下贡献：

1. 这是首个基于单目、双目和RGB-D相机的开源SLAM方案，这个方案包括，回环检测、地图重用和重定位。

2. 我们的RGB-D结果说明，**光速法平差优化（BA）比ICP或者光度和深度误差最小方法的更加精确**。

3. 通过匹配远处和近处的双目匹配的点和单目观测，我们的双目的结果比直接使用双目系统更加精确。

4. 针对无法建图的情况，提出了一个轻量级的定位模式 ，能够更加有效的重用地图。

![img](http://img.mp.sohu.com/upload/20170703/7bb2440c527943f9beb1e258e8808f48_th.png)

​                                            (a)双目输入：带有多次回环检测的城市环境轨迹和稀疏重建

![img](http://img.mp.sohu.com/upload/20170703/f7ba323bfff04a12ae5318c84b3d1f0a.png)

(b) RGB-D输入：房间关键帧和稠密点云已经一次回环检测图，这些点云通过对深度图的关键帧的位姿进行映射得到，不进行渲染(融合)

图1 是ORB-SLAM2处理双目和RGB-D输入评估相机的轨迹并建图。这个系统能够保证在高精度和鲁棒性的前提下，做到在标准CPU上进行实时的，回环检测，重定位以及地图重用。

图a中显示的是双目和RGB输入下的ORBSLAM2的输出。双目例子显示的是最后轨迹和稀疏重建的地图。这里的数据集来源于KITTI的Sequence00数据集。这个城市数据集是ORB-SLAM2多次成功提取特征，并且回环检测而来。

RGB-D例子是来源于TUM 的RGB-D 数据库中的fr1_room的数据集，并且进行关键帧的位姿评估而来。通过评估关键帧的位姿，映射深度图，最终形成一个稠密的点云图。指的注意的一点是，ORB-SLAM2虽不像Kinect Fusion一样进行数据融合，但是却能够很精确的估计关键帧的位姿。更多的例子在附件视频中展示。在余下的篇章当中，我们将会在第二部分讨论相关的工作。在第三部分谈论ORB-SLAM2系统框架。第四部分评价ORB-SLAM2，第五部分得出结论。

## II. 相关工作

在这个章节，我们将会讨论双目和RGB-D SLAM的相关工作。评估部分我们放在第四部分，本章我们主要讨论的是SLAM的方法。

### 2.1 双目SLAM

最早研究双目SLAM方案的是Paz 等人[5]，基于条件独立分割和扩展SLAM，其显著特点是能够在大场景中运行。更重要的是，这是第一个使用近特征点和远特征点（例如，由于双目相机差异较小，导致点的深度不能准确的估计）的双目SLAM系统，使用一个逆深度参数进行估计。经验值表明如果深度小于40倍双目的基线，那么这个点就能被三角测量化。我们就是跟随的这样思想来处理远近不同的特征点，具体解释放在第三部分。

目前大多数双目系统都是基于特征匹配和局部BA优化的方式，来获得尺度。Strasdat等人[8]采用在一个输出窗口的关键帧[7]和位姿的BA联合优化算法。在全局不一致性的情况下，通过限制窗口的大小的方式，实现了约束了时间的复杂程度的目的。Mei等人[9]在限定时间复杂度的条件下，使用路标和位姿相关性的方式的实现了RSLAM解决方案，并且提出和实现了在活动的区域的BA优化算法。即使在全局不一致的条件下，RSLAM也能够进行闭环，同时会扩大回环两侧的活动区域。

Pire等人[10]把局部的BA运用到了邻近S-PTAM上面来，但是，这种方法缺少大量的回环检测。与此相似的是，我们对局部关键帧采用BA优化，因此，这个地图的大小和复杂程度的大小是独立的，进而，我们可以在一个大场景当中运行。然而，我们目标是建立一个全局不变的地图。因此，我们的系统首先在回环的两端执行。这与RSLAM很相似，以便于能够使用旧的地图进行定位，之后进行位姿估计，即将回环产生的累积漂移最小化。

Engel等人[11]提出邻近双目LSD-SLAM方案，采用的是一种直接的半稠密方法，最小化==高==梯度的图像区域中的光度误差。这种方法希望能够在不依赖特征提取的条件下，能够在纹理不清或者模糊运动的过程中获得更高的鲁棒性。然而，直接法的性能会由于滚动（卷帘）快门，或者非朗伯反射的未建模的因素影响而下降。

### 2.2 RGB-D SLAM

最早和最著名的RGB-DSLAM系统是有Newcombe等人[4]提出的KinectFusion，这种方法将深度数据进行融合，深度数据来源于传感器到深度模型，常常使用ICP算法来跟踪相机的位姿。由于==体积的表现形式==使用体素表示和缺乏回环检测，这种算法只能工作在小的工作空间。Whelan 等人[12]提出的Kintinuous能够在大环境中运行。它通过使用一个滚动循环缓冲器和包括使用位置定位和位姿优化来达到回环检测的目的。

第一个开源的==RGB-DSLAM==RGBD-SLAM方案是由Endres[13]提出的，这是一种基于特征点提取的系统，他的前端采用提取和匹配特征点和ICP来计算帧与帧之间的运动。

后端采用位姿图优化的方式，回环检测约束条件来源于一个启发式搜索。相似的是，Kerl 等人[14]提出的DVO-SLAM，是在关键帧与关键帧之间的优化位姿图，视觉里程计通过计算最小化光度和深度误差来计算约束条件。DVO-SLAM同时在以前的所有帧当中，搜索回环的候选者，而不依赖于位置识别。

Whelan等人[15]提出的邻近ElasticFusion算法，是建立在基于确定环境的地图。这是一种以地图为中心的方法。这种方法忽略了非刚性形变地图的位姿和回环检测的性能，也是不是一个标准的位姿图优化。这种方法在重建和定位的精度都是十分优秀的，但是目前的应用十分有限对于一个房间大小的地图，由于在地图当中面元的数量影响计算的复杂程度。

Strasdat等人[8]提出ORB-SLAM2这种方法，这个方法使用深度信息去合成一个三维坐标，能够精确的提取到一副图像的信息。ORB-SLAM2能够处理来自双目和RGB-D的图像，与上述方法不同的是，**我们的后端是用的BA算法**，来建立一个**全局的稀疏的地图**重建，因此我们的方法更加轻量级并且能够在标准的CPU上面运行。我们的目标是长时间并且全局精准定位，而不是建立一个有很多细节的稠密地图。然而，高精度的关键帧的位姿，能够融合深度图像以及在计算中得到精准的重建，或者能够处理所有的关键帧和深度图，以及所有的BA并且得到一个精准的3D模型。

## III. ORBSLAM2

针对双目相机和RGB-D相机的ORB-SLAM2建立在单目ORB-SLAM的基础上，它的核心组件，如图2所示。

![img](http://img.mp.sohu.com/upload/20170703/af9978fc454c45599c39bd5aceb1c03b.png)

![orbslam](C:\Users\AVML224-07\Desktop/orbslam.jpg)

图2 ORB-SLAM2由三个==平行==并行的线程组成，**跟踪，局部建图和回环检测**。**在一次回环检测后，会执行第四个线程，去执行BA优化**。跟踪的线程在双目或者RGB-D输入之前进行，因此剩下的系统模块能够跟传感器模块独立运行。单目的ORB-SLAM2工作图也是这幅图。

这个系统主要有3个并行的线程：

1、通过寻找局部地图的特征匹配，以及只运用BA算法来最小化重投影误差，进行跟踪和定位每帧的相机。

2、运用局部的BA算法完成局部建图并且优化。

3、回环检测检能够通过执行位姿图的优化来更正累计漂移误差。在位姿图优化之后，会启动第四个线程来执行全局BA算法，来计算整个系统最优结构和运动的结果。

这个系统有一个**基于DBoW2[16]**的嵌入式位置识别模型，来达到重定位，防止跟踪失败（如遮挡），和已知地图的场景重初始化，以及回环检测的目的。这个系统维持着连接任意两个关键帧的共同点的共视图[8]，以及连接所有关键帧的最小生长树。这些关键帧的图结构能够得到一个关键帧的局部窗口，以便于跟踪和局部建图，并且允许在大型的环境中工作，且在回环检测中作为一种图优化的结构。

这个系统使用相同的ORB特征进行跟踪、建图和位置识别的任务。这些特征在旋转不变和尺度不变性上有良好的鲁棒性，同时对相机的自动增益、曝光和光线的变化表现出良好的稳定性。并且能够迅速的提取特征和进行匹配，能够满足实时操作的需求，能够在基于词袋模型的位置识别过程中，显示出良好的精度[18]。

在本章的剩下的部分当中，我将会展示双目以及深度信息是如何利用，和系统中的哪些部分会受其影响。对每个系统块更详尽的描述，可参见论文[1]

### 3.1 单目、近处双目和远处双目特征点

ORB-SLAM2作为一种基于特征提取的方法，预处理输入以提取显著关键点位置处的特征，如图2b所示。系统的所有运行都是基于输入图像的特征展开，而不依赖于双目或者RGB-D的相机。我们的系统处理单目或者双目的特征点，分成远处特征点和近处特征点两类。

双目特征点 通过三个坐标$x_s=(u_L,v_L,u_R)$定义。当中，$(u_L,v_L)$是左边图像的坐标，$u_R$是右边图像的水平坐标。对于双目相机而言，我们提取两幅图像当中的ORB特征，对于每个左边的ORB特征我们对其匹配到右边的图像中。这对于建设双目图像校正十分有效，因此极线是水平的。之后我们会在左边的图像产生双目的ORB特征点，和一条水平的线向右边的图像进行匹配，通过修补相关性来重新定义亚像素。对于RGB-D相机，正如Strasdat等人[8]所言，我们提取在图像通道$(u_L,v_L)$上提取ORB特征点，我们将其深度值$d$转换到一个虚拟的右坐标中，
$$
u_R=u_L-\frac{f_xb}{d}
$$
其中，$f_x$是水平焦距，$b$是结构光投影器和红外相机的基线，kinect和华硕 Xtion 我们近似为8cm。我们将深度值和已经处理的深度地图，和基线在结构光投影器和红外相机进行匹配，对每一帧的图像与右边图像的坐标系进行融合。这是kinect和华硕 Xtion 精度大约是8cm。深度传感器的不确定性由虚拟右坐标的不确定性表示。 通过这种方式，双目和RGB-D输入的信息由系统的其余部分同等处理。

近双目特征点的定义是：匹配的深度值小于40倍双目/RGB-D的基线，否则的话，是远特征点。当深度信息估计精准时，近的特征点能够从一帧的深度值实现三角测量化，并且能够提供尺度、平移和旋转的信息。另外一方面，远的特征点，能够提供精确的旋转信息，但是更差的尺度和平移信息。当提供多视图的时候，我们才能三角化那些远的点。

单目特征点由左图像上的两个坐标$x_m=(u_L,v_L)$定义，相当于双目中所有ORB无法找到匹配的情况或者是在RGB-D中具有无效深度值的情况。这些点仅能够从多视图中三角测量化并且不能够提供尺度信息，但是可以提供旋转和平移的估计信息。

### 3.2 系统引导

使用双目和RGB-D相机的主要优势在于，我们可以直接从一帧图像中获得深度信息，我们不需要像单目情况中那样做一个特定的运动结构（SFM）初始化。在系统初始化的时候，我们就创造了一个关键帧（也就是第一帧），将他的位姿进行初始化，从所有的立体点中创造一个初始化地图。

### 3.3 使用单目或者双目==光束优化法==光束平差法

我们的系统采用光束优化法（BA），优化在跟踪过程（纯运动BA）中相机的位姿，优化本地窗口的关键帧和局部地图的特征点（局部BA），并且在回环检测之后优化所有的关键帧和特征点（全局BA）。**我们在g2o当中使用Levenberg-Marquadt方法**[19]。

纯运动BA，优化相机旋转矩阵$R\in SO(3)$和位置$t\in R^3$，最小化世界坐标系下匹配的3D点云$X^i\in R^3$和特征点$x^i_{(·)}$（单目$x^i_m\in R^2$的或双目的$x^i_s\in R^3$，其中$i\in \chi$的重投影误差：

![img](http://img.mp.sohu.com/upload/20170703/db6f803451e5401ea9ead1ab523539ae.png)

在这个式子当中，$\rho$是鲁棒的Huber cost函数，$\Sigma$是协方差矩阵，与特征点的尺度相关。这个投影函数$\pi _{(·)}$，单目的时候使用$\pi _{(m)}$，修正双目的时候用$\pi _{(s)}$,他们的定义如下：

![img](http://img.mp.sohu.com/upload/20170703/b4642b2d8f824489be4da5a2a9c3d203.png)

在这个式子当中$(f_x,f_y)$是焦距，$(c_x,c_y)$是主要点（象点），b是基线，所有的这些参数都是通过标定获得。

局部BA 优化一系列可用的关键帧$\kappa_L$和所有在这些关键帧中的可观点$P_L$，所有的其他关键帧$\kappa_F$，即不在$\kappa_L$，考虑$P_L$当中的特征点有助于代价函数，但是在优化中是固定的。定义$\chi_k$为$P_L$和关键帧$k$的一系列匹配特征点，这个优化问题如下：

![img](http://img.mp.sohu.com/upload/20170703/3fad0dd7b2cb4089874578fb6d3f6360.png)

全局BA是 局部光束法的一个特例，这个方法除了初始帧所有的关键帧和点在地图当中都会被优化.初始帧是固定的，用来消除随机化。

### 3.4 回环检测和全局BA

回环检测有两步：首先，一个回环信息被确定检测到，然后利用这个回环纠正和优化位姿图。相比于单目的ORB-SLAM中可能出现尺度漂移的地方[20]，这个双目或者深度的信息将会使得尺度信息可观测。并且，几何校验和位姿图优化将不再需要处理尺度漂移，而且是基于刚体变换的，而不是基于相似性。

在ORB-SLAM2的位姿优化后，我们包含一个全局的BA优化。这个优化方案可能计算量非常巨大，我们必须采用一个独立的线程，允许系统能够持续的建图，并且检测回环信息。但是这将会再次触发全局BA优化与当前地图的合成。如果在优化运行时检测到新的循环，我们将中止优化并继续完成回环，这将再次启动完整的BA优化。当完整的BA结束时，我们需要将更新的关键帧子集和由完整BA优化的点与未更新的关键帧和在优化运行时插入的点合并。最后通过生成树将校正后的更新关键帧（例如，这个变换从未优化到已优化）传播到一个未更新关键帧中。根据校正参考帧来转换这些未更新的特征点。

### 3.5 关键帧的插入

ORB-SLAM2遵循在单目ORB-SLAM中提的法则，即经常插入关键帧并且剔除上一帧的冗余。远近特征点的差异为我们插入一个新的关键帧提供了条件，这在大场景的条件下是至关重要的，如图3所示。

![img](http://img.mp.sohu.com/upload/20170703/bec5e3d3328242f98071f2526d2e6cc0.png)

图3 KITTI01中的跟踪点。高速公路的跟踪点。绿色的特征点深度小于40倍双目的基线，蓝色特征点大于40倍双目的基线，在这种数据集当中，需要插入大量的关键帧，以便于有足够的近处特征点来完成精确的位移估计。远处的特征点来估计方向，但是不能够计算平移和尺度。

在这样的环境中，我们需要大量的近点用以精确估计平移，因而如果跟踪近点的数量小于$\tau_t$并且这一帧可以创造$\tau_c$个新邻近立体点，那么这个系统将会插入一个新的关键帧。经验值认为，当$\tau_t=100$和$\tau_c=70$的条件下效果最好。

### 3.6 定位模式

ORB-SLAM2包括一个定位模式，该模式适用于轻量级以及在地图已知情况下长期运行，只要那个环境没有发生剧烈变化。在该模式中，局部建图和回环检测的线程中是停用的，并且这个相机始终都是在通过追踪进行重定位。在这个模式下，追踪模块使用视觉里程计进行匹配图像的点云。视觉里程计对当前帧的ORB算子和由双目或者深度相机收集的3D点云进行匹配。这些匹配使得在没有地图的区域也能够精确重新定位，但是漂移将会被累加。地图点云匹配确保了在一个已经存在的地图当中零漂移定位。这个模型在附带的video当中会显示。

## IV. 评估

我们已经在三个流行的数据集中评估了ORB-SLAM2，并与其他最先进的SLAM系统进行了比较，使用原始作者发布的结果和文献中的标准评估指标。我们在一台16G的RAM，Intel Core i7-4790的台式机运行。为了消除多线程系统的不确定性，我们运行每个序列五次，并显示估计轨迹精度的中值结果。我们的开源实现包括在所有这些数据集中运行系统的标定和指令。

### 4.1 KITTI数据集

KITTI数据集包含从在城市和高速公路环境中运行的汽车那里采集的双目数据。双目传感器具有小于54厘米的基线，并且在分辨率为1240×376像素的校正之后以10Hz频率工作。其中序列00,02,05,06,07和09包含回环。我们的ORB-SLAM2能够检测出几乎所有回环并且能够在此之后实现地图重用，除了09序列以外，09序列的回环只发生在尾端少数的几帧当中。表1显示了11个训练数据的结果，这是一个公开的真实数据，对比于原先的LSD-SLAM算法，我们展示了的双目SLAM系统测试数据结果。我们使用两个不同的度量方式，绝对平移均方根误差$t_{abs}$（在论文[3]中提到），与平均相对平移$t_{rel}$和旋转误差$r_{rel}$（在论文[2]中提到）。我们的系统在大多数序列当中比双目的LSD-SLAM要优秀很多，并且能够获得低于1%的相对误差。这个序列01，如图3所示，是训练集中唯一个高速公路的序列，其平移误差表现稍差。平移在这个序列当中是难以评估的，因为由于很高的速度和较低的帧率导致只有很少几个近点能够被跟踪到。然而方向能够被精确的评估，获得的误差是每100米0.21度，因为很多远点能够被长期跟踪。如图4所示，显示了一些评估的例子。

![img](http://img.mp.sohu.com/upload/20170703/433379e2b58e4682bf782709e7593e4e_th.png)

表1两种SLAM在测试==KITT，I==KITTI数据的精度对比

![img](http://img.mp.sohu.com/upload/20170703/3a76e767eca74f09be80095479a65a64_th.png)

图4 在==KITTE==KITTI数据集01,05,07和08数据集，估计轨迹（黑色线）和以及实际运动（红色线）

与[1]中提出的单目结果相比，所提出的双目版本能够处理单目系统失败的序列01。 在这个高速公路序列中，参见图3，近点仅在几帧中可见。 双目版本有着仅从一个立体关键帧就可以创建点云的能力，而不是依靠单目的延迟初始化（包括在两个关键帧之间找到匹配），在这个序列中是至关重要的，不会丢失跟踪。 此外，双目系统用米尺估计地图和轨迹，并且不受尺度漂移的影响，如图5所示。

==![图5](C:\Users\AVML224-07\Desktop\图5.jpg)==

图5. KITTI 08中的估计轨迹（黑色）和地面实况（红色）。左：单目ORB-SLAM [1]，右：ORB-SLAM2（双目）。 单目ORBSLAM在此序列中有严重的尺度漂移，特别是在转弯时。 相比之下，所提出的双目版本能够在没有比例漂移的情况下估计轨迹和地图的真实比例。

### 4.2 EuRoC 数据集

EuRoC 数据集包含了11个双目的序列，数据通过一个微型飞行器（MAV）在两个不同的房间和一个大型工业环境中采集。这个双目传感器有一个小于11cm的基线，并能够提供20HZ的高分辨率图像。这个序列分成，简单、中等和困难，这取决于MAV（微型飞行器）的速度，照明和场景的纹理。在所有的序列当中，MAV（微型飞行器）再次访问这个环境的时候，ORB-SLAM2能够重用地图，在必要时可以进行回环检测。这个表格2显示的是ORB-SLAM2在所有序列中的绝对平移的最小均方误差。并与文献11中采用双目LSD-SLAM得到的结果相比较。

![img](http://img.mp.sohu.com/upload/20170703/2079bba44ed6499e923d579e4634a57f_th.png)

ORB-SLAM2能够实现一个厘米级精准的定位，并且比双目的LSD-SLAM更加的精确。由于严重的运动模糊，跟踪部分可能会在V2_ 03_ difficul 序列当中部分丢失。在文献22中，这个序列可以通过IMU信息进行处理。图6显示的是一些估计轨迹与真实的情况相对比的例子。

![img](http://img.mp.sohu.com/upload/20170703/236836d0806843899a09c4890ce0df34_th.png)

图6在EuRoC V1_02_medium，V2_02_medium，MH_03_medium和MH_05_数据集测试结果，其中估计轨迹（黑色）实际运动轨迹（红色）

### 4.3 TUM RGB-D数据集

TUM RGB-D数据集[3]包含来自RGB-D传感器的室内序列，这些传感器分组在几个类别中，以评估不同纹理，照明和结构条件下的物体重建和SLAM /视觉里程计的性能。和大多数RGB-DSLAM方法一样，我们将实验结果展示在一个序列子集当中。在表格3当中，我们将我们的精度与以下最先进的方法进行比较，例如ElasticFusion，Kintinuous，DVO-SLAM以及RGB-DSLAM。ORB-SLAM2是唯一一种基于光束流差法，并且在大多数序列中比其他的方法都更加优秀。我们已经注意到RGB-DSLAM在文献1中的结果，深度地图对于freiburg2序列有一个4%的尺度误差，误差可能来自错误的标定，我们已经在运行过程中，进行了一定程度的补偿，这能够部分解释我们取得好的结果的原因。图7显示出了四个序列中通过从计算出的关键帧姿势反向投影传感器深度图而得到的点云。书桌和海报的良好清晰度和直线轮廓证明了我们方法的高精度定位。

![img](http://img.mp.sohu.com/upload/20170703/76a11c05ecec4dd492ce6d1741671ffe_th.png)

图6 TUM RGB-D数据的fr3office, fr1 room, fr2 desk and fr3 nst 序列的通过评估关键帧的位姿和深度图进行稠密的点云重建图

### 4.4 计时结果

为了完成对所提出的系统的评估，我们在表IV中呈现了具有不同图像分辨率和传感器的三个序列的计时结果。 显示每个线程任务的平均值和两个标准偏差范围。 由于这些序列包含一个单回环，因此完整的BA和回环检测线程的一些任务仅执行一次，并且仅报告单个时间测量。 每帧的平均跟踪时间低于每个序列的相机帧速率的倒数，这意味着我们的系统能够实时工作。 由于立体图像中的ORB提取是并行化的，可以看出在V2_02的双目高分辨率图像中提取1000个ORB特征类似于在fr3_office的单个VGA图像通道中提取相同数量的特征。

回环中关键帧的数量显示为与回环检测相关的时间的参考。 虽然KITTI 07中的回环包含更多关键帧，但作为室内fr3_office构建的共视图更密集，因此，回环融合、姿势图优化和完整BA任务更加耗时。共视图的较高密度使得局部地图包含更多关键帧和点，因此，局部地图跟踪和局部BA也更耗时。

==![表4](C:\Users\AVML224-07\Desktop\表4.jpg)==

## V. 结论

我们为单目，双目和RGB-D传感器提供了一个完整的SLAM系统，能够在标准CPU上实时执行重定位、回环检测、重用其地图等功能。 我们专注于构建全局一致的地图，以便在各种环境中提长时间且可靠的定位，如实验中所示。 所提出的具有系统重定位能力的定位模式是对于已知环境而言非常稳健、零漂移和轻量级定位方法。 此模式对于某些应用程序非常有用，例如在良好映射的空间中跟踪虚拟现实中的用户视角。

与现有技术的比较表明，ORB-SLAM2在大多数情况下实现了最高的精度。 在KITTI视觉里程计基准测试中，ORB-SLAM2是目前最好的双目SLAM解决方案。 至关重要的是，与近年来蓬勃发展的立体视觉测距方法相比，ORB-SLAM2在已经建图区域实现了零漂移定位。

令人惊讶的是，我们的RGB-D结果表明，如果需要最精确的相机定位，则BA法比直接方法或ICP更好，另外的优点是计算成本较低，不需要GPU处理来实时操作。

我们已经发布了我们系统的源代码，其中包含示例和说明，以便其他研究人员可以轻松使用。ORB-SLAM2是我们所知的第一个开源视觉SLAM系统，可以使用单目、双目和RGB-D输入。此外，我们的源代码包含使用单目相机的增强现实应用2的示例，以显示我们的解决方案的潜力。

未来扩展可能包括，举一些例子，非重叠多视角摄像机，鱼眼或全向摄像机，大规模密集融合，以及联合建图或者增加运动模糊的鲁棒性。

